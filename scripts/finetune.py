import argparse
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.utils.prune as prune
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import os

# Import model c·ªßa b·∫°n (ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ƒë√∫ng)
# N·∫øu b·∫°n ƒë·ªÉ model trong src/model.py th√¨ s·ª≠a l·∫°i import cho ph√π h·ª£p
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from src.model_ann import VisionTransformer 

def get_args():
    parser = argparse.ArgumentParser(description='Fine-tune Pruned ANN')
    parser.add_argument('--checkpoint_path', type=str, required=True, help='Path to best ANN checkpoint')
    parser.add_argument('--save_path', type=str, default='./checkpoints/ann_pruned_finetuned.pth')
    parser.add_argument('--pruning_ratio', type=float, default=0.2, help='Amount of sparsity (e.g. 0.2 = 20%)')
    parser.add_argument('--epochs', type=int, default=15, help='Number of fine-tuning epochs')
    parser.add_argument('--lr', type=float, default=1e-5, help='Low learning rate for fine-tuning')
    parser.add_argument('--batch_size', type=int, default=128)
    
    # Ki·∫øn tr√∫c Model (Ph·∫£i kh·ªõp v·ªõi model c≈©)
    parser.add_argument('--embed_dim', type=int, default=192)
    parser.add_argument('--depth', type=int, default=12)
    parser.add_argument('--heads', type=int, default=3)
    
    return parser.parse_args()

def apply_pruning(model, amount):
    """
    √Åp d·ª•ng Pruning nh∆∞ng KH√îNG remove mask ngay.
    Vi·ªác gi·ªØ mask gi√∫p PyTorch bi·∫øt nh·ªØng tr·ªçng s·ªë n√†o c·∫ßn gi·ªØ l√† 0 trong l√∫c train.
    """
    print(f"‚úÇÔ∏è Applying pruning mask with ratio: {amount}...")
    parameters_to_prune = []
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            # B·ªè qua l·ªõp classifier head ƒë·ªÉ gi·ªØ ƒë·ªô ch√≠nh x√°c cao nh·∫•t
            if "head" in name: 
                continue
            parameters_to_prune.append((module, 'weight'))
    
    # D√πng Global Unstructured Pruning
    prune.global_unstructured(
        parameters_to_prune,
        pruning_method=prune.L1Unstructured,
        amount=amount,
    )
    return parameters_to_prune

def make_pruning_permanent(parameters_to_prune):
    """
    Sau khi train xong, ta '√©p' mask v√†o tr·ªçng s·ªë th·∫≠t ƒë·ªÉ l∆∞u file cho g·ªçn.
    """
    print("üîí Making pruning permanent...")
    for module, name in parameters_to_prune:
        prune.remove(module, name)

def main():
    args = get_args()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")

    # 1. Load Data (CIFAR-10)
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.TrivialAugmentWide(), # D√πng Augmentation nh·∫π
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)

    # 2. Kh·ªüi t·∫°o & Load Model G·ªëc
    print(f"Loading checkpoint from {args.checkpoint_path}")
    model = VisionTransformer(
        dim=args.embed_dim, depth=args.depth, heads=args.heads, num_classes=10
    ).to(device)
    
    checkpoint = torch.load(args.checkpoint_path, map_location=device)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)

    # 3. √Åp d·ª•ng Pruning (T·∫°o Mask)
    pruning_params = apply_pruning(model, args.pruning_ratio)
    
    # 4. Setup Optimizer cho Fine-tuning
    # L∆∞u √Ω: Learning Rate ph·∫£i R·∫§T NH·ªé (1e-5 ho·∫∑c 5e-5) ƒë·ªÉ kh√¥ng ph√° h·ªèng weight ƒëang t·ªët
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()

    # 5. Training Loop (Fine-tuning)
    model.train()
    print(f"üöÄ Start Fine-tuning for {args.epochs} epochs...")
    
    for epoch in range(args.epochs):
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
        
        acc = 100. * correct / total
        print(f"Epoch {epoch+1}/{args.epochs} | Loss: {total_loss/len(train_loader):.4f} | Acc: {acc:.2f}%")

    # 6. Ch·ªët ƒë∆°n (L∆∞u model)
    # Tr∆∞·ªõc khi l∆∞u ph·∫£i remove mask ƒë·ªÉ bi·∫øn 0 ·∫£o th√†nh 0 th·∫≠t
    make_pruning_permanent(pruning_params)
    
    # Ki·ªÉm tra ƒë·ªô th∆∞a (Sparsity)
    total_zeros = 0
    total_params = 0
    for name, m in model.named_modules():
        if isinstance(m, nn.Linear):
            total_zeros += torch.sum(m.weight == 0).item()
            total_params += m.weight.nelement()
    print(f"‚úÖ Final Model Sparsity: {100. * total_zeros / total_params:.2f}%")

    # L∆∞u file
    torch.save({'model_state_dict': model.state_dict()}, args.save_path)
    print(f"üíæ Model saved to {args.save_path}")

if __name__ == '__main__':
    main()